> Home assignment for **Senior Big Data Engineer â€“ AI-Focused Data Infrastructure**

RAG Pipeline
RAG Pipeline is a Retrieval-Augmented Generation (RAG) project that scrapes book data, processes it through multiple ETL stages, generates embeddings for semantic search, and exposes a FastAPI-based API to query the knowledge base. The pipeline uses MinIO as object storage, Apache Airflow for orchestration, and ChromaDB for vector embeddings.

Project Overview
â€¢	Scraper: Scrapes book data from https://books.toscrape.com website.
â€¢	ETL Pipeline:
â€¢	  - RAW: Raw text files from scraper.
â€¢	  - BRONZE: Cleaned Parquet files.
â€¢	  - SILVER: Enhanced Parquet files with word counts.
â€¢	  - GOLD: Embedded text chunks stored in ChromaDB vector store.
â€¢	Data Quality Checks: Runs automated data quality validations on SILVER data.
â€¢	FastAPI Service: Provides an API endpoint for querying the RAG knowledge base.
â€¢	Airflow: Orchestrates the pipeline DAG.


Folder Structure
rag_pipeline
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ Dockerfile.api
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ rag_pipeline.py
â”‚   â””â”€â”€ airflow.db
â”œâ”€â”€ architecture.png
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ lineage/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ silver/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ downloads/
â”œâ”€â”€ mydata/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ silver/
â”œâ”€â”€ rag_api.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â”œâ”€â”€ data_quality.py
â”‚   â”œâ”€â”€ etl.py
â”‚   â”œâ”€â”€ rag_utils.py
â”‚   â””â”€â”€ scraper.py
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â””â”€â”€ test_scraper.py


âš™ï¸ Setup Instructions
Prerequisites
â€¢	Docker and Docker Compose installed
â€¢	Python 3.11 (for local runs)
â€¢	MinIO credentials set in .env file

Environment Variables
Create a .env file with the following content (already included in your project):
MINIO_URL=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=mydata
FERNET_KEY=l2ckboQEnybXHIMXgwprr_GSQUwkzyuqJ0R2ZFunTLY=
SECRET_KEY=l2ckboQEnybXHIMXgwprr_GSQUwkzyuqJ0R2ZFunTLY=
CHROMA_DIR=/opt/data/gold/chroma


ğŸ³ Running with Docker Compose
Build and start all services (MinIO, API, Airflow):
$ make docker-up

Stop and remove all containers:
$ make docker-down

Start Airflow components:
$ make airflow_up

View Airflow scheduler and webserver logs:
$ make airflow_logs

Trigger the Airflow DAG manually:
$ make airflow_dag_trigger

Airflow UI will be available at http://localhost:8080
FastAPI API will be available at http://localhost:8000
MinIO Console UI available at http://localhost:9001 (login with your MinIO credentials)


ğŸ§° Running Locally (without Docker)
### 1. Clone and configure
```bash
git clone https://github.com/sabrinsingh/home_assignment_rag_pipeline.git
cd rag-pipeline
Create a Python virtual environment and install dependencies:
$ python3 -m venv env
source env/bin/activate
pip install -r requirements.txt

ollama serve

Run scraper:
$ make scrape

Run full ETL pipeline:
$ make etl

Start FastAPI server:
$ make api

Run tests:
$ make test

ğŸ§© Makefile Commands
Command	Description
make scrape	Run scraper to collect raw book data
make etl	Run full ETL pipeline (RAW â†’ GOLD + DQ)
make api	Start FastAPI server locally
make docker-up	Build and start Docker containers
make docker-down	Stop and remove Docker containers
make airflow_up	Start Airflow webserver, scheduler, init
make airflow_dag_trigger	Trigger Airflow DAG manually
make airflow_logs	Tail Airflow scheduler and webserver logs
make test	Run pytest tests
make clean	Remove temporary files
ğŸ“„ API Usage
Root
GET /
Returns welcome message.
Query Endpoint
POST /query/
Content-Type: application/json

{
  "query": "Your question here"
}

Response:
{
  "question": "Your question here",
  "answer": "Generated answer from RAG",
  "context": "Context documents used"
}


ğŸ› ï¸ Project Components
â€¢	Scraper (src/scraper.py): Scrapes book info from the web and uploads raw text to MinIO.
â€¢	ETL Pipeline (src/etl.py): Processes data through RAW â†’ BRONZE â†’ SILVER â†’ GOLD stages.
â€¢	Data Quality Checks (src/data_quality.py): Performs checks like nulls, duplicates, empty strings.
â€¢	Vector DB: Uses ChromaDB to store sentence embeddings for fast retrieval.
â€¢	API (rag_api.py): FastAPI app serving RAG answers with local vector DB and Ollama LLM.
â€¢	Airflow DAG (airflow/dags/rag_pipeline.py): Orchestrates scraping and ETL tasks.


ğŸ–¼ï¸ Architecture Diagram
Refer to architecture.png in the project root folder.

Feel free to open issues or pull requests for improvements or bug fixes.

ğŸ‘¨â€ğŸ’» Author
Sabrin Lal Singh
sabrinlalsingh@gmail.com
